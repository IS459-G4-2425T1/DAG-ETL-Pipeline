{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c14b18",
   "metadata": {},
   "source": [
    "# Regression with Amazon SageMaker XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U sagemaker\n",
    "!pip install pyarrow fastparquet\n",
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af1ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done import\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import boto3\n",
    "from boto3 import session\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "print(\"done import\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0ada65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Load Training Data\n",
    "bucket_name = 'pipeline2-data-storage'\n",
    "file_name = 'part-00003-9a62fedd-6eb0-4804-a32b-302a11cd6a91-c000.snappy.parquet'\n",
    "file_key = 'historic-processed/df-cascade/'+ file_name\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "transfer = S3Transfer(s3)\n",
    "transfer.download_file(bucket_name, file_key, file_name)\n",
    "\n",
    "# Load Parquet file into a DataFrame\n",
    "table = pq.read_table(file_name)\n",
    "df = table.to_pandas()\n",
    "\n",
    "\n",
    "\n",
    "# # Sample 1000000 records for training due to resource limitations\n",
    "df = df.sample(n=1000000, random_state=42)\n",
    "      \n",
    "print(\"Total number of records:\", df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e61697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff12db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.208885   6.1441884 14.356416  ... 11.580776   6.9681387 11.414705 ]\n",
      "Mean Absolute Error: 20.71948922856197\n",
      "Root Mean Squared Error: 35.9846092948558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['crsdeptime'] = df['crsdeptime'].astype(str).str.zfill(4)  # Ensure all times are in \"HHMM\"\n",
    "df['crsdephour'] = df['crsdeptime'].str[:2].astype(int)       # Extract hour as integer\n",
    "df['crsdepminute'] = df['crsdeptime'].str[2:].astype(int)     # Extract minute as integer\n",
    "\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "X = df[['dayofmonth', 'year', 'month', 'origin', 'dest', 'crsdephour','crsdepminute', 'uniquecarrier']]\n",
    "\n",
    "y = df['arrdelay']\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "X = pd.get_dummies(X, columns=['origin', 'dest', 'uniquecarrier'])\n",
    "\n",
    "# Assuming your preprocessed data is in X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',  # For regression tasks\n",
    "    n_estimators=100,              # Number of trees\n",
    "    learning_rate=0.1,             # Step size shrinkage\n",
    "    max_depth=6,                   # Maximum depth of a tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92754efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch to optimize model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "                           param_grid, scoring='neg_mean_absolute_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c884930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_model.pkl uploaded to s3://pipeline2-data-storage/models/xgboost_model.pkl\n",
      "feature_columns.pkl uploaded to s3://pipeline2-data-storage/models/feature_columns.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model to S3\n",
    "# Save the features to S3 as well\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Define bucket name and file name in S3\n",
    "bucket_name = 'pipeline2-data-storage'\n",
    "model_path = 'models/xgboost_model.pkl'\n",
    "columns_path = 'models/feature_columns.pkl'\n",
    "\n",
    "# Save the trained model locally as a .pkl file\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "\n",
    "# Save the feature columns locally as a .pkl file\n",
    "joblib.dump(X.columns, 'feature_columns.pkl')\n",
    "\n",
    "# Function to upload file to S3\n",
    "def upload_file_to_s3(local_path, bucket, s3_path):\n",
    "    with open(local_path, 'rb') as file_data:\n",
    "        s3.upload_fileobj(file_data, bucket, s3_path)\n",
    "    print(f\"{local_path} uploaded to s3://{bucket}/{s3_path}\")\n",
    "\n",
    "# Upload the model and feature columns to S3\n",
    "upload_file_to_s3('xgboost_model.pkl', bucket_name, model_path)\n",
    "upload_file_to_s3('feature_columns.pkl', bucket_name, columns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b85ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'flightnum': 123, 'orgin': 'ABE', 'destination': 'ABI', 'carrier': 'B6', 'predicted_delay': 3.362814426422119}, {'flightnum': 345, 'orgin': 'ABI', 'destination': 'ABQ', 'carrier': 'DL', 'predicted_delay': 2.300037145614624}, {'flightnum': 678, 'orgin': 'ABQ', 'destination': 'CDC', 'carrier': 'F9', 'predicted_delay': -0.5688889622688293}]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with mock data mimicking live API data input\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        'flightnum':123,\n",
    "        'origin':'ABE',\n",
    "        'dest':'ABI',\n",
    "        'uniquecarrier': 'B6',\n",
    "        'deptime': 1620,\n",
    "        'dayofmonth':5,\n",
    "        'year':2019,\n",
    "        'month':3,\n",
    "    },\n",
    "    {\n",
    "        'flightnum':345,\n",
    "        'origin':'ABI',\n",
    "        'dest':'ABQ',\n",
    "        'uniquecarrier': 'DL',\n",
    "        'deptime': 1620,\n",
    "        'dayofmonth':12,\n",
    "        'year':2012,\n",
    "        'month':12,\n",
    "    },\n",
    "    {\n",
    "        'flightnum':678,\n",
    "        'origin':'ABQ',\n",
    "        'dest':'CDC',\n",
    "        'uniquecarrier': 'F9',\n",
    "        'deptime': 1620,\n",
    "        'dayofmonth':2,\n",
    "        'year':2003,\n",
    "        'month':6,\n",
    "    }\n",
    "]\n",
    "\n",
    "new_data_df = pd.DataFrame(test_data)\n",
    "flightnums = new_data_df['flightnum']\n",
    "origins = new_data_df['origin']\n",
    "dests = new_data_df['dest']\n",
    "uniquecarriers = new_data_df['uniquecarrier']\n",
    "\n",
    "\n",
    "# Process the input to match training features\n",
    "# Ensure 'crsdeptime' has 4 characters by padding with leading zeros if necessary\n",
    "new_data_df['deptime'] = new_data_df['deptime'].astype(str).str.zfill(4)\n",
    "# new_data_df['deptime'] = new_data_df['deptime'].str[:2].astype(int)\n",
    "new_data_df['crsdepminute'] = new_data_df['deptime'].str[2:].astype(int)\n",
    "new_data_df = new_data_df.drop(columns=['deptime','flightnum'])\n",
    "new_data_df = pd.get_dummies(new_data_df, columns=['origin', 'dest', 'uniquecarrier'])\n",
    "\n",
    "# Reindex to match training feature columns\n",
    "new_data_df = new_data_df.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Make predictions for all rows\n",
    "predictions = xgb_model.predict(new_data_df)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "        'flightnum': flightnums,\n",
    "        'orgin': origins,\n",
    "        'destination': dests,\n",
    "        'carrier': uniquecarriers,\n",
    "        'expected_delay': predictions\n",
    "    })\n",
    "\n",
    "# Convert result to JSON format for return\n",
    "result_json = result_df.to_dict(orient='records')\n",
    "print(result_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d9a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
